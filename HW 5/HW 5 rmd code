---
title: "HW 5"
author: "Hailey Flo"
date: "03/27/2024"
output: 
  pdf_document:
    number_sections: true
---

This homework is meant to give you practice in creating and defending a position with both statistical and philosophical evidence.  We have now extensively talked about the COMPAS ^[https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis] data set, the flaws in applying it but also its potential upside if its shortcomings can be overlooked.  
We have also spent time in class verbally assessing positions both for an against applying this data set in real life.  In no more than two pages ^[knit to a pdf to ensure page count] take the persona of a statistical consultant advising a judge as to whether they should include the results of the COMPAS algorithm in their decision making process for granting parole.  
First clearly articulate your position (whether the algorithm should be used or not) and then defend said position using both statistical and philosophical evidence.  Your paper will be grade both on the merits of its persuasive appeal but also the applicability of the statistical and philosohpical evidence cited.  


Your Honor, I believe the COMPAS algorithm should not be used in your decision making process for granting parole. This algorithm processes a number of variables about an individual in order to determine their likelihood of recidivism, or re-offending. While this information may sound helpful, there are actually numerous underlying issues that make it unfit for official use in the criminal justice system and in your adjudication.

The first and perhaps most pressing issue is that this dataset perpetuates existing biases in our legal system. To use it would be to contribute towards greater long-term harms. According to statistical review, the algorithm often predicts black defendants to be at a higher risk of recidivism than they actually are. 
Forty-five percent of black individuals were falsely misclassified as being high risk compared to an error rate of only twenty-eight percent for white offenders. This means that black people are being incorrectly labeled as high-risk nearly twice as often This significant misclassification rate makes incorporating the algorithm in the final decision making process highly unethical. 
The system only has an overall accuracy rate of 61%, meaning 39% of all defendants are rated incorrectly. Also, you are being given a false report nearly forty percent of the time. 

According to the law of consequentialism, the use of this method is immoral because of the negative impact it has on many individuals who would otherwise be granted parole. The intent of the program is to prevent additional crimes from occurring and protect the general public, however, it is difficult to say for certain whether a greater number of people are benefiting from it than there are defendants that are unjustifiably harmed in being refused parole. 
Moreover, it has been statistically proven that a large percentage of violent white re-offenders were released, upon COMPAS’s suggestion, because the algorithm mistakenly lowers one’s reported likelihood of violent recidivism if an individual is white. Labeling black defendants as high risk and white defendants as low risk primarily on the basis of their race creates stigmas surrounding future offers of parole; 
and it would be extremely dangerous should our justice system begin to generalize individuals in this manner. Continuous use of this program will lead to systematic bias, even unintentionally, over time by exacerbating stereotypes and racial discrimination in our legal system.

While COMPAS’s developer, Northpointe Inc., has stated that race is not a variable taken into account, it can include variables that indirectly reveal race–including ZIP code. In the case of more extreme offenses, future violent recidivism is analyzed according to factors including prior crimes and one’s age and gender. 
An analysis of the algorithm found that even when controlling for these variables, black defendants were seventy-seven percent more likely to be assigned a higher risk score than white defendants. Race should not be the ultimate determinant for predicting violent crimes as indicated by this inequity. 
Clearly, this algorithm is using racially-impacted proxies as factors that skew the accuracy of final results and create proxy discrimination. Your decision should be based on factual evidence from the documented defendant’s criminal record, not a racist and somewhat poor estimation generated by a computer system.

There is also an issue of transparency within the production and functionality of this algorithm. Northpointe has not revealed the exact variables considered within its rating process, nor who or what is responsible for designing the algorithm itself. The process is essentially a “black box,” meaning that it is unable to be completely audited, replicated, or analyzed. 
This makes your job as the judge more difficult, in that you will not be able to identify the exact stakeholders involved when presenting results. From the framework of virtue ethics, this would arguably present a significant ethical concern in which you would be violating the virtue of honesty. Moreover, the algorithm’s lack of transparency means you would be unable to provide conclusive justification for any COMPAS results in the court of law. 
As such, you could be held accountable for utilizing its reports in your decision making–especially if your judgment is swayed by an erroneous rating. To avoid being held accountable for the suffering of others, you should refrain from using the COMPAS algorithm in your decision making process.

Lastly, using the COMPAS algorithm within the criminal justice system violates an individual’s right to due processing. As previously mentioned, the statistical results indicate that the algorithm is stereotyping defendants based on certain variables, including race, age, and gender. Analysts developed a logistic regression model that considered race, age, criminal history, future recidivism, 
charge degree, gender and age, and concluded that both race and age were definitive predictors of higher risk scores. For example, defendants under the age of twenty-five were 2.5 times as likely to get a high risk rating than those who were middle-aged. You will agree that it is unfair to categorize people based on attributes that they have no control over. 
Each individual should be judged on a case-by-case basis, taking into account their current behavior, openness to reform, and any extenuating circumstances in their situation. A judge’s role is to exercise personal judgment and fully deliberate the points of every case. While the COMPAS algorithm could be viewed as no more than a tool, in reality it presents a true threat to one’s right of due process. 
It can easily impact a judge’s final decision regarding an offer of parole, without any limitations or safeguards in place to ensure an adequate reviewal. It is important that you view all defendants as human beings, not just a series of numbers or statistics. 

Please consider these many concerns when deciding whether to include the results of the COMPAS algorithm in your decisions for parole. There are a myriad of moral issues pertaining to this computer system, not the least of which is that it will enhance racial stereotypes and biases within our criminal justice system. The merits of it as a supporting tool do not outweigh the many statistical errors found in its results, namely amongst black and white individuals. 
Notably, the algorithm has an overall accuracy rate of just 61%, which should not be great enough to justify utilizing in your decision. Judges should strive to reach a well-informed, accurate, and fair conclusion for everyone in the court of law. Using the COMPAS algorithm in your decision making process would be both irresponsible and an abuse of power, ultimately diminishing the trust one has in our honorable legal system.
