---
title: "HW 4"
author: "Hailey Flo"
date: "03/18/2024"
output: 
  html_document:
    number_sections: true
---

This homework is designed to give you practice fitting a logistic regression and working with statistical/philosophical measures of fairness.  We will work with the `titanic` dataset which we have previously seen in class in connection to decision trees.  

Below I will preprocess the data precisely as we did in class.  You can simply refer to `data_train` as your training data and `data_test` as your testing data.  


```{r}

#this is all of the preprocessing done for the decision trees lecture.  

path <- 'https://raw.githubusercontent.com/guru99-edu/R-Programming/master/titanic_data.csv'
titanic <-read.csv(path)
head(titanic)

library(dplyr)

#replace ? with NA
replace_question_mark <- function(x) {
  if (is.character(x)) {
    x <- na_if(x, "?")
  }
  return(x)
}

titanic <- titanic %>%
  mutate_all(replace_question_mark)

set.seed(678)
shuffle_index <- sample(1:nrow(titanic))
head(shuffle_index)

titanic <- titanic[shuffle_index, ]
head(titanic)

library(dplyr)
# Drop variables
clean_titanic <- titanic %>%
select(-c(home.dest, cabin, name, x, ticket)) %>% 
#Convert to factor level
    mutate(pclass = factor(pclass, levels = c(1, 2, 3), labels = c('Upper', 'Middle', 'Lower')),
    survived = factor(survived, levels = c(0, 1), labels = c('No', 'Yes'))) %>%
na.omit()
#previously were characters
clean_titanic$age <- as.numeric(clean_titanic$age)
clean_titanic$fare <- as.numeric(clean_titanic$fare)
glimpse(clean_titanic)

create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
data_train <- create_train_test(clean_titanic, 0.8, train = TRUE)
data_test <- create_train_test(clean_titanic, 0.8, train = FALSE)

```

#
Create a table reporting the proportion of people in the training set surviving the Titanic.  Do the same for the testing set.  Comment on whether the current training-testing partition looks suitable.  

```{r}
prop.table(table(data_train$survived))
prop.table(table(data_test$survived))
```

Yes, the current training-testing partition looks suitable. The proportion of people who survived the titanic in the training set is 0.398 and 0.444 in the testing set. This is only a .046 difference in proportion, making it a representable partition.

#
Use the `glm` command to build a logistic regression on the training partition.  `survived` should be your response variable and `pclass`, `sex`, `age`, `sibsp`, and `parch` should be your response variables.  

```{r}
model<- glm(survived ~ pclass+sex+age+fare+sibsp+parch, family = binomial(link = "logit"), data=data_train)
model

```

We would now like to test whether this classifier is *fair* across the sex subgroups.  It was reported that women and children were prioritized on the life-boats and as a result survived the incident at a much higher rate.  
Let us see if our model is able to capture this fact.  

#

Subset your test data into a male group and a female group.  Then, use the `predict` function on the male testing group to come up with predicted probabilities of surviving the Titanic for each male in the testing set.  Do the same for the female testing group.  

```{r}
test_data_male <- subset(data_test, sex == 'male')
test_data_female <- subset(data_test, sex == 'female')


male.predict.results <- predict(model,newdata=test_data_male,type='response')
female.predict.results <- predict(model,newdata=test_data_female,type='response')

```

# 

Now recall that for this logistic *regression* to be a true classifier, we need to pair it with a decision boundary.  Use an `if-else` statement to translate any predicted probability in the male group greater than $0.5$ into `Yes` 
(as in Yes this individual is predicted to have survived).  Likewise an predicted probability less than $0.5$ should be translated into a `No`.  

Do this for the female testing group as well, and then create a confusion matrix for each of the male and female test set predictions.  You can use the `confusionMatrix` command as seen in class to expidite this process as well as provide you necessary metrics for the following questions.  

```{r}
library(caret)

male.fitted.results <- ifelse(male.predict.results > 0.5,"Yes","No")
female.fitted.results <- ifelse(female.predict.results > 0.5,"Yes","No")

female_survive_mat <- confusionMatrix(as.factor(female.fitted.results), test_data_female$survived, positive = "Yes")
male_survive_mat <- confusionMatrix(as.factor(male.fitted.results), test_data_male$survived, positive = "Yes")

female_survive_mat
male_survive_mat
```

#
We can see that indeed, at least within the testing groups, women did seem to survive at a higher proportion than men (24.8\% to 76.3\% in the testing set).  Print a summary of your trained model and interpret one of the fitted coefficients in light of the above disparity.  

```{r}
summary(model)
```

The coefficient 'sexmale' shows that the estimated survival rate per unit, so long as all other coefficients stay the same, will be multiplied e^-2.669752, or 0.0692. This means that if the person belongs to the male category, their survival odds will significantly decrease. 


#

Now let's see if our model is *fair* across this explanatory variable.  Calculate five measures (as defined in class) in this question: the Overall accuracy rate ratio between females and males, the disparate impact between females and males, 
the statistical parity between females and males, and the predictive equality as well as equal opportunity between females and males (collectively these last two comprise equalized odds).  Set a reasonable $\epsilon$ each time and then comment on which (if any) of these five criteria are met.  


```{r}
Accuracy_female<-female_survive_mat$overall[[1]]
Accuracy_male<-(male_survive_mat$overall[[1]])
Accuracy_ratio <- Accuracy_male/Accuracy_female
Accuracy_ratio

female_positives <- (female_survive_mat$table[2,1]+female_survive_mat$table[2,2])/(female_survive_mat$table[1,1] + female_survive_mat$table[1,2] + female_survive_mat$table[2,1] + female_survive_mat$table[2,2])
male_positives <- (male_survive_mat$table[2,1]+male_survive_mat$table[2,2])/(male_survive_mat$table[1,1] + male_survive_mat$table[1,2] + male_survive_mat$table[2,1] + male_survive_mat$table[2,2])

disparate_impact<- male_positives/female_positives
disparate_impact

statistical_parity<-female_positives-male_positives
statistical_parity

true_odds_female<- (female_survive_mat$table[2,2])/(female_survive_mat$table[2,2]+female_survive_mat$table[1,2])
true_odds_male<- (male_survive_mat$table[2,2])/(male_survive_mat$table[2,2]+male_survive_mat$table[1,2])
false_odds_female<-(female_survive_mat$table[2,1])/(female_survive_mat$table[2,1]+female_survive_mat$table[1,1])
false_odds_male<-(male_survive_mat$table[2,1])/(male_survive_mat$table[2,1]+male_survive_mat$table[1,1])
equalized_opportunity <- true_odds_female - true_odds_male
equalized_opportunity

predictive_equality<- false_odds_female - false_odds_male
predictive_equality
```
The overall accuracy rate between male and female is 0.964, and with an epsilon of 0.2 this is not statistically problematic. The percent difference between the two accuracy rates is less than 0.2, so the accuracy criteria is met. Disparate impact will also have an epsilon of 0.2, so it will need to be smaller than 1 - 0.2, or 0.8. 
The calculated disparate impact is 0.073, which is less than 0.8, so this criteria is also met. The third measure is statistical parity which will also have an epsilon of 0.2. The difference in statistical parity between males and females is 0.857 which is larger than 0.2, illustrating that the criteria is not met and there is a statistical issue with demographics. 
The fourth measure, equalized opportunity has an epsilon of 0.2 and a value of 0.829. This measure also does not meet the criteria given that 0.829 is larger than 0.2. Lastly, predictive equality with an epsilon of 0.2 is once again over this criteria level at 0.745, indicating that predictive equality is not satisfied.



It is always important for us to interpret our results in light of the original data and the context of the analysis.  In this case, it is relevant that we are analyzing a historical event post-facto and any disparities across demographics identified are unlikely to be replicated.  So even though our model fails numerous of the statistical fairness criteria, 
I would argue we need not worry that our model could be misused to perpetuate discrimination in the future.  After all, this model is likely not being used to prescribe a preferred method of treatment in the future.  


#

Even so, provide a *philosophical* notion of justice or fairness that may have motivated the Titanic survivors to act as they did. Spell out what this philosophical notion or principle entails?

One philsophical notion of justice that may have motivated the Titanic survivors to act as they did was justice as need. The priniciple of justice as need entails providing resources based on individual needs. Similar to John Rawls' belief that the vulnerable should be most protected, more resources were allocated to women and children because they were deemed more vulnerable. 
The men on the titanic felt morally obligated to save the ones who were less likely to survive without help, and sought to make sure they got on the rafts first. Because women and children were more in need, not having body strength or warmth, they were given more resources.

